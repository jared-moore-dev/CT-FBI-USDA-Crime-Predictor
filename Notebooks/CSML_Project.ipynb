{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S8YLqk_BZyzj",
    "outputId": "c4014322-a099-43e7-ed21-b5ccb392c532"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting pandas==2.0.3\n",
      "  Downloading pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3) (2024.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3) (1.26.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas==2.0.3) (1.16.0)\n",
      "Downloading pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m12.3/12.3 MB\u001B[0m \u001B[31m30.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: pandas\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.2.2\n",
      "    Uninstalling pandas-2.2.2:\n",
      "      Successfully uninstalled pandas-2.2.2\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.0.3 which is incompatible.\n",
      "mizani 0.13.0 requires pandas>=2.2.0, but you have pandas 2.0.3 which is incompatible.\n",
      "plotnine 0.14.1 requires pandas>=2.2.0, but you have pandas 2.0.3 which is incompatible.\n",
      "xarray 2024.10.0 requires pandas>=2.1, but you have pandas 2.0.3 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[0mSuccessfully installed pandas-2.0.3\n",
      "--2024-11-30 05:54:38--  https://ucr.fbi.gov/nibrs/2019/tables/state-excels/connecticut_offense_type_by_agency_2019.xls/output.xls\n",
      "Resolving ucr.fbi.gov (ucr.fbi.gov)... 104.16.148.244, 104.16.149.244, 2606:4700::6810:95f4, ...\n",
      "Connecting to ucr.fbi.gov (ucr.fbi.gov)|104.16.148.244|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 119296 (116K) [application/vnd.ms-excel]\n",
      "Saving to: ‘connecticut_offense_type_by_agency_2019.xls’\n",
      "\n",
      "connecticut_offense 100%[===================>] 116.50K  --.-KB/s    in 0.002s  \n",
      "\n",
      "2024-11-30 05:54:38 (59.3 MB/s) - ‘connecticut_offense_type_by_agency_2019.xls’ saved [119296/119296]\n",
      "\n",
      "--2024-11-30 05:54:38--  https://www.ers.usda.gov/webdocs/DataFiles/80591/FoodAccessResearchAtlasData2019.xlsx?v=8276.1\n",
      "Resolving www.ers.usda.gov (www.ers.usda.gov)... 20.141.137.224\n",
      "Connecting to www.ers.usda.gov (www.ers.usda.gov)|20.141.137.224|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 85803176 (82M) [application/vnd.openxmlformats-officedocument.spreadsheetml.sheet]\n",
      "Saving to: ‘FoodAccessResearchAtlasData2019.xlsx’\n",
      "\n",
      "FoodAccessResearchA 100%[===================>]  81.83M  85.5MB/s    in 1.0s    \n",
      "\n",
      "2024-11-30 05:54:40 (85.5 MB/s) - ‘FoodAccessResearchAtlasData2019.xlsx’ saved [85803176/85803176]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas==2.0.3\n",
    "import pandas as pd\n",
    "\n",
    "# Download our datasets and load them to dataframes\n",
    "!wget -O connecticut_offense_type_by_agency_2019.xls \"https://ucr.fbi.gov/nibrs/2019/tables/state-excels/connecticut_offense_type_by_agency_2019.xls/output.xls\"\n",
    "CT_Crime_df = pd.read_excel('connecticut_offense_type_by_agency_2019.xls', skiprows=4, nrows=87)\n",
    "\n",
    "# Correct the column names for the crime dataframe\n",
    "crime_columns = [\n",
    "    \"Agency Type\", \"Agency Name\", \"Population\", \"Total Offenses\",\n",
    "    \"Crimes Against Persons\", \"Crimes Against Property\", \"Crimes Against Society\",\n",
    "    \"Assault Offenses\", \"Aggravated Assault\", \"Simple Assault\", \"Intimidation\",\n",
    "    \"Homicide Offenses\", \"Murder and Nonnegligent Manslaughter\", \"Negligent Manslaughter\",\n",
    "    \"Justifiable Homicide\", \"Human Trafficking Offenses\", \"Commercial Sex Acts\",\n",
    "    \"Involuntary Servitude\", \"Kidnapping/Abduction\", \"Sex Offenses\", \"Rape\",\n",
    "    \"Sodomy\", \"Sexual Assault With an Object\", \"Fondling\", \"Incest\", \"Statutory Rape\",\n",
    "    \"Arson\", \"Bribery\", \"Burglary/Breaking & Entering\", \"Counterfeiting/Forgery\",\n",
    "    \"Destruction/Damage/Vandalism of Property\", \"Embezzlement\", \"Extortion/Blackmail\",\n",
    "    \"Fraud Offenses\", \"False Pretenses/Swindle/Confidence Game\",\n",
    "    \"Credit Card/Automated Teller Machine Fraud\", \"Impersonation\", \"Welfare Fraud\",\n",
    "    \"Wire Fraud\", \"Identity Theft\", \"Hacking/Computer Invasion\", \"Larceny/Theft Offenses\",\n",
    "    \"Pocket-picking\", \"Purse-snatching\", \"Shoplifting\", \"Theft From Building\",\n",
    "    \"Theft From Coin Operated Machine or Device\", \"Theft From Motor Vehicle\",\n",
    "    \"Theft of Motor Vehicle Parts or Accessories\", \"All Other Larceny\", \"Motor Vehicle Theft\",\n",
    "    \"Robbery\", \"Stolen Property Offenses\", \"Animal Cruelty\", \"Drug/Narcotic Offenses\",\n",
    "    \"Drug/Narcotic Violations\", \"Drug Equipment Violations\", \"Gambling Offenses\",\n",
    "    \"Betting/Wagering\", \"Operating/Promoting/Assisting Gambling\",\n",
    "    \"Gambling Equipment Violations\", \"Sports Tampering\", \"Pornography/Obscene Material\",\n",
    "    \"Prostitution Offenses\", \"Prostitution\", \"Assisting or Promoting Prostitution\",\n",
    "    \"Purchasing Prostitution\", \"Weapon Law Violations\"\n",
    "]\n",
    "\n",
    "# Reassign these to the dataframe\n",
    "CT_Crime_df.columns = crime_columns\n",
    "\n",
    "!wget -O FoodAccessResearchAtlasData2019.xlsx \"https://www.ers.usda.gov/webdocs/DataFiles/80591/FoodAccessResearchAtlasData2019.xlsx?v=8276.1\"\n",
    "CT_Food_df = pd.read_excel('FoodAccessResearchAtlasData2019.xlsx',\n",
    "                                     sheet_name='Food Access Research Atlas',\n",
    "                                     skiprows=range(1, 12818),  # Skip rows 1 to 12818\n",
    "                                     nrows=828)                 # Read 828 rows (13647 - 12819)\n",
    "# Grab rows 61238-61243\n",
    "Austin_County_Food_df = pd.read_excel('FoodAccessResearchAtlasData2019.xlsx',\n",
    "                                     sheet_name='Food Access Research Atlas',\n",
    "                                     skiprows=range(1, 61237),  # Skip rows 1 to 61238\n",
    "                                     nrows=6)                 # Read 6 rows (61238-61243)\n",
    "\n",
    "!wget -O tract2town-2020.xlsx \"https://github.com/CT-Data-Collaborative/ct-census-tract-to-town/blob/master/2020/tract2town-2020.xlsx?raw=true\"\n",
    "CT_Town_df = pd.read_excel('tract2town-2020.xlsx')\n",
    "\n",
    "with pd.ExcelWriter('raw_output.xlsx') as writer:\n",
    "    CT_Crime_df.to_excel(writer, sheet_name='CT_Crime')\n",
    "    CT_Food_df.to_excel(writer, sheet_name='CT_Food')\n",
    "    CT_Town_df.to_excel(writer, sheet_name='CT_Town')\n",
    "    Austin_County_Food_df.to_excel(writer, sheet_name='Austin_County_Food')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Cleanup / Sanitization\n",
    "- CT Town / Fips Conversion sheet\n",
    " - Add Missing Willimantic and Grotton Town data based on external researched data\n",
    " -\n",
    "\n",
    "- CT Food Data\n",
    " - Add values for blanks (mostly 0's from data analysis)\n",
    " - Remove columns where all values are blank\n",
    " - Remove columns where there is population % next to raw data\n",
    " - Add Town column based on FIPS code\n",
    "    - Reorder town to be 3rd column\n",
    "\n",
    " - Aggregate Tract by tract data into Town by Town Data\n",
    "\n",
    "- CT Crime Data\n",
    "  - Rename Columns to sensible names\n",
    "  - Drop the agency type column\n",
    " - Make a version that is normalized via the population\n",
    "    - Divide each number of crimes by the town population to get per capita crimes"
   ],
   "metadata": {
    "id": "_sgJ3r7m0Udk"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "### CT Town / Fips Conversion sheet  Cleanup ###\n",
    "\n",
    "# Clean CT_Town Groton and Willimantic data to include those towns\n",
    "# Define relevant columns\n",
    "Town_columns = ['tract_fips', 'tract_name', 'town', 'county', 'town_fips']\n",
    "willimantic_fips = ['9015800300', '9015800600', '9015800400', '9015800700']\n",
    "grottontown_fips = ['9011702100', '9011702800', '9011702900', '9011703000', '9011702300', '9011702600', '9011702700', '9011980000', '9011870200']\n",
    "grottonpropper_fips = ['9011702400', '9011702500']\n",
    "\n",
    "\n",
    "\n",
    "# Ensure that tract_fips is treated as a string\n",
    "CT_Town_df['tract_fips'] = CT_Town_df['tract_fips'].astype(str)\n",
    "\n",
    "# Print initial state to verify current values\n",
    "print(\"Before updates:\")\n",
    "print(CT_Town_df[CT_Town_df['tract_fips'].isin(willimantic_fips + grottontown_fips+grottonpropper_fips)])\n",
    "\n",
    "# Update Groton Town\n",
    "for fips in grottontown_fips:\n",
    "    CT_Town_df.loc[CT_Town_df['tract_fips'] == fips, 'town'] = 'Groton Town'\n",
    "\n",
    "# Update Willimantic\n",
    "for fips in willimantic_fips:\n",
    "    CT_Town_df.loc[CT_Town_df['tract_fips'] == fips, 'town'] = 'Willimantic'\n",
    "\n",
    "# Print state after updates to verify changes\n",
    "print(\"After updates:\")\n",
    "print(CT_Town_df[CT_Town_df['tract_fips'].isin(willimantic_fips + grottontown_fips+grottonpropper_fips)])\n",
    "\n",
    "# Export to Excel\n",
    "CT_Town_df.to_excel('townrevised.xlsx', index=False)\n"
   ],
   "metadata": {
    "id": "Hl329HJjAeLl"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "### Data Cleanup Functions ###\n",
    "## Functions:\n",
    "def fill_blanks_with_zero(df):\n",
    "  df = df.fillna(0)\n",
    "  return df\n",
    "\n",
    "def remove_all_zero_columns(df):\n",
    "  # Transpose the DataFrame to treat columns as rows\n",
    "  df_transposed = df.T\n",
    "\n",
    "  # Drop rows (originally columns) with all zeros, ignoring the first row (header)\n",
    "  df_transposed = df_transposed.drop(df_transposed[(df_transposed[df_transposed.columns[1:]].sum(axis=1) == 0)].index)\n",
    "\n",
    "  # Transpose back to original shape\n",
    "  df = df_transposed.T\n",
    "  return df\n",
    "\n",
    "def add_town_by_fips(Food_df, Town_df):\n",
    "    # Ensure CensusTract and tract_fips columns have the same data type for matching\n",
    "    Food_df['CensusTract'] = Food_df['CensusTract'].astype(str)\n",
    "    Town_df['tract_fips'] = Town_df['tract_fips'].astype(str)\n",
    "\n",
    "    # Merge the two DataFrames based on the CensusTract and tract_fips columns\n",
    "    Food_df = pd.merge(Food_df, Town_df[['tract_fips', 'town']], left_on='CensusTract', right_on='tract_fips', how='left')\n",
    "\n",
    "    # Drop the 'tract_fips' column from the merged DataFrame\n",
    "    Food_df.drop(columns=['tract_fips'], inplace=True)\n",
    "\n",
    "    # Remove rows with no town value\n",
    "    Food_df.dropna(subset=['town'], inplace=True)\n",
    "\n",
    "    # Get the index of the 'State' column\n",
    "    state_index = Food_df.columns.get_loc('State')\n",
    "\n",
    "    # Insert the 'town' column at the desired position (state_index)\n",
    "    Food_df.insert(state_index, 'town', Food_df.pop('town'))\n",
    "\n",
    "    return Food_df\n",
    "\n",
    "def remove_population_share_columns(df):\n",
    "  # Get a list of columns containing \"share\"\n",
    "  share_columns = [col for col in df if 'share' in col]\n",
    "\n",
    "  # Remove those columns from the DataFrame\n",
    "  df = df.drop(columns=share_columns)\n",
    "  return df\n",
    "\n",
    "def aggregate_town_rows(df):\n",
    "  df = df.groupby('town', as_index=False, dropna=True).agg({\n",
    "      # 'CensusTract': 'first',\n",
    "      'State': lambda x: x.mode()[0] if not x.mode().empty else None,\n",
    "      'County': lambda x: x.mode()[0] if not x.mode().empty else None,\n",
    "      'Urban': 'mean',\n",
    "      'Pop2010': 'sum',\n",
    "      'OHU2010': 'sum',\n",
    "      'GroupQuartersFlag': 'mean',\n",
    "      'NUMGQTRS': 'sum',\n",
    "      'PCTGQTRS': 'mean',\n",
    "      'LILATracts_1And10': 'mean',\n",
    "      'LILATracts_halfAnd10': 'mean',\n",
    "      'LILATracts_1And20': 'mean',\n",
    "      'LILATracts_Vehicle': 'mean',\n",
    "      'HUNVFlag': 'mean',\n",
    "      'LowIncomeTracts': 'mean',\n",
    "      'PovertyRate': 'mean',\n",
    "      'MedianFamilyIncome': 'mean',\n",
    "      'LA1and10': 'mean',\n",
    "      'LAhalfand10': 'mean',\n",
    "      'LA1and20': 'mean',\n",
    "      'LATracts_half': 'mean',\n",
    "      'LATracts1': 'mean',\n",
    "      'LATractsVehicle_20': 'mean',\n",
    "      'LAPOP1_10': 'sum',\n",
    "      'LAPOP05_10': 'sum',\n",
    "      'LAPOP1_20': 'sum',\n",
    "      'LALOWI1_10': 'sum',\n",
    "      'LALOWI05_10': 'sum',\n",
    "      'LALOWI1_20': 'sum',\n",
    "      'lapophalf': 'sum',\n",
    "      'lalowihalf': 'sum',\n",
    "      'lakidshalf': 'sum',\n",
    "      'laseniorshalf': 'sum',\n",
    "      'lawhitehalf': 'sum',\n",
    "      'lablackhalf': 'sum',\n",
    "      'laasianhalf': 'sum',\n",
    "      'lanhopihalf': 'sum',\n",
    "      'laomultirhalf': 'sum',\n",
    "      'lahisphalf': 'sum',\n",
    "      'lahunvhalf': 'sum',\n",
    "      'lasnaphalf': 'sum',\n",
    "      'lapop1': 'sum',\n",
    "      'lalowi1': 'sum',\n",
    "      'lakids1': 'sum',\n",
    "      'laseniors1': 'sum',\n",
    "      'lawhite1': 'sum',\n",
    "      'lablack1': 'sum',\n",
    "      'laasian1': 'sum',\n",
    "      'lanhopi1': 'sum',\n",
    "      'laaian1': 'sum',\n",
    "      'laomultir1': 'sum',\n",
    "      'lahisp1': 'sum',\n",
    "      'lahunv1': 'sum',\n",
    "      'lasnap1': 'sum',\n",
    "      'TractLOWI': 'sum',\n",
    "      'TractKids': 'sum',\n",
    "      'TractSeniors': 'sum',\n",
    "      'TractWhite': 'sum',\n",
    "      'TractBlack': 'sum',\n",
    "      'TractAsian': 'sum',\n",
    "      'TractNHOPI': 'sum',\n",
    "      'TractAIAN': 'sum',\n",
    "      'TractOMultir': 'sum',\n",
    "      'TractHispanic': 'sum',\n",
    "      'TractHUNV': 'sum',\n",
    "      'TractSNAP': 'sum',\n",
    "  })\n",
    "  # Get the column names\n",
    "  cols = df.columns.tolist()\n",
    "\n",
    "  # Move the first column to the third position\n",
    "  cols.insert(2, cols.pop(0))\n",
    "\n",
    "  # Reindex the DataFrame\n",
    "  df = df.reindex(cols, axis=1)\n",
    "\n",
    "  return df"
   ],
   "metadata": {
    "id": "Dl6kCQO4QNTT"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    " ### CT Food Data Cleanup ###\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def clean_CT_Food(df, town_df):\n",
    "  # Assuming CT_Food_df is your DataFrame\n",
    "  CT_Food_df = df\n",
    "  CT_Town_df = town_df\n",
    "\n",
    "  # Display the original DataFrame\n",
    "  print(\"Original DataFrame:\")\n",
    "  print(CT_Food_df)\n",
    "\n",
    "  print(\"Original DataFrame shape:\", CT_Food_df.shape)\n",
    "\n",
    "  CT_Food_revised_df = CT_Food_df.copy()\n",
    "\n",
    "  # Add values in blanks (mostly 0's)\n",
    "  CT_Food_revised_df = fill_blanks_with_zero(CT_Food_revised_df)\n",
    "  print(\"Shape after fill_blanks_with_zero:\", CT_Food_revised_df.shape)\n",
    "\n",
    "  CT_Food_revised_df = remove_all_zero_columns(CT_Food_revised_df)\n",
    "  print(\"Shape after remove_all_zero_columns:\", CT_Food_revised_df.shape)\n",
    "\n",
    "  CT_Food_revised_df = remove_population_share_columns(CT_Food_revised_df)\n",
    "  print(\"Shape after remove_population_share_columns:\", CT_Food_revised_df.shape)\n",
    "\n",
    "  CT_Food_revised_df = add_town_by_fips(CT_Food_revised_df, CT_Town_df)\n",
    "  print(\"Shape after add_town_by_fips:\", CT_Food_revised_df.shape)  # Check shape here!\n",
    "  print(\"Dropped Unincorperated tracts / Tracts without Towns\")\n",
    "\n",
    "  print(CT_Food_revised_df.head())\n",
    "\n",
    "  CT_Food_revised_df.to_excel('foodrevised.xlsx', index=False)\n",
    "\n",
    "  CT_Food_Aggregated = aggregate_town_rows(CT_Food_revised_df)\n",
    "  CT_Food_Aggregated.to_excel('foodaggregated.xlsx', index=False)\n",
    "\n",
    "  return CT_Food_revised_df, CT_Food_Aggregated\n",
    "\n",
    "CT_Food_revised, CT_Food_Aggregated = clean_CT_Food(CT_Food_df, CT_Town_df)"
   ],
   "metadata": {
    "id": "HJBNcdwpAHu0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "### CT Crime Data Cleanup\n",
    "\n",
    "def clean_CT_Crime(df):\n",
    "    # Drop the 'Agency Type' column\n",
    "    df = df.drop(columns=['Agency Type'])\n",
    "\n",
    "    return df\n",
    "\n",
    "def normalize_crime_data(df):\n",
    "\n",
    "  # Create a copy of the DataFrame to avoid modifying the original\n",
    "  normalized_df = df.copy()\n",
    "\n",
    "  # Select the columns to normalize (excluding 'Agency Name' and 'Population')\n",
    "  columns_to_normalize = df.columns[2:]  # Starts from the third column\n",
    "\n",
    "  # Normalize the selected columns by dividing by the population\n",
    "  normalized_df[columns_to_normalize] = normalized_df[columns_to_normalize].div(normalized_df['Population'], axis=0)\n",
    "\n",
    "  return normalized_df\n",
    "\n",
    "CT_Crime_revised = clean_CT_Crime(CT_Crime_df)\n",
    "CT_Crime_revised.to_excel('crimerevised.xlsx', index=False)\n",
    "CT_Crime_normalized = normalize_crime_data(CT_Crime_revised)\n",
    "CT_Crime_normalized.to_excel('crimenormalized.xlsx', index=False)"
   ],
   "metadata": {
    "id": "zyVw98RVWT5v"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Enrichment (Combine the data)\n",
    "- Combine CT Crime Revised + CT Food Aggregated\n",
    "    - CT Crime data has fewer times than CT Food Aggregated, Drop towns in aggregated that we dont have crime statistics for\n",
    "    - Combine the rows of data where agency name in crime_revised = town in ct_food_aggregated\n",
    "- Stretch CT_Crime_normalized over CT_Food_Revised"
   ],
   "metadata": {
    "id": "Gpr2HmJ3AJDz"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "## Combine CT Crime Data and Aggregated\n",
    "def combine_crime_food_data(crime_df, food_df):\n",
    "\n",
    "  # Get a list of towns present in the crime data\n",
    "  crime_towns = crime_df['Agency Name'].unique()\n",
    "\n",
    "  # Filter the food data to keep only towns present in the crime data\n",
    "  filtered_food_df = food_df[food_df['town'].isin(crime_towns)]\n",
    "\n",
    "  # Merge the two DataFrames based on town/agency name\n",
    "  merged_df = pd.merge(filtered_food_df, crime_df, left_on='town', right_on='Agency Name', how='inner')\n",
    "\n",
    "  # Remove the 'Agency Name' column\n",
    "  merged_df.drop(columns=['Agency Name'], inplace=True)\n",
    "\n",
    "  # Reorder columns to place food data on the left and crime data on the right\n",
    "  food_cols = filtered_food_df.columns.tolist()  # Get food columns\n",
    "  crime_cols = [col for col in merged_df.columns if col not in food_cols]  # Get crime columns\n",
    "  reordered_cols = food_cols + crime_cols  # Combine columns in desired order\n",
    "  merged_df = merged_df[reordered_cols]  # Reorder the DataFrame\n",
    "\n",
    "  return merged_df\n",
    "\n",
    "combined_crime_food_df = combine_crime_food_data(CT_Crime_revised, CT_Food_Aggregated)\n",
    "combined_crime_food_df.to_excel('combined.xlsx', index=False)\n",
    "\n"
   ],
   "metadata": {
    "id": "_xsVRxWXeKiy"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "\n",
    "## Data Transformation\n",
    "- SVD to reduce the dimensionality of the data\n",
    "- TS-SNE to explore data relationships\n",
    "\n",
    "## Predition / Machine Learning\n",
    "- Use the Food Data to predict Total Crime numbers\n",
    "- Use the Food Data to predict likelihoods of specific types of crimes\n",
    "\n",
    "## Plotting Data / Visualization\n",
    "- Crime Plot\n",
    "- Relationship diagram"
   ],
   "metadata": {
    "id": "AgTkI3Hps_PJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "# Assuming combined_crime_food_df is already defined from the previous code\n",
    "\n",
    "# 1. Correlation Matrix Heatmap:\n",
    "# Visualize the correlation between different food security and crime statistics.\n",
    "selected_columns = ['LowIncomeTracts', 'PovertyRate', 'MedianFamilyIncome', # Food security financial indicators\n",
    "                    'Urban', 'OHU2010', 'PCTGQTRS', 'lapophalf', 'lapop1', 'lahunv1', # Food security Locational Indicators\n",
    "                    'TractLOWI', 'TractKids', 'TractSeniors', 'TractWhite', 'TractBlack', 'TractAsian', 'TractNHOPI', 'TractAIAN', 'TractOMultir', 'TractHispanic', 'TractHUNV', 'TractSNAP', # Food Security Demographic Indicators\n",
    "                    'Total Offenses', 'Crimes Against Persons', 'Crimes Against Property', 'Crimes Against Society', 'Assault Offenses', 'Homicide Offenses', 'Human Trafficking Offenses', 'Sex Offenses', 'Fraud Offenses', 'Robbery', 'Drug/Narcotic Offenses', 'Drug/Narcotic Violations', 'Gambling Offenses', 'Prostitution Offenses', 'Weapon Law Violations']  # Crime statistics (add more as needed)\n",
    "\n",
    "subset_df = combined_crime_food_df[selected_columns]\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = subset_df.corr()\n",
    "\n",
    "# Mask the upper triangle (including the diagonal)\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "\n",
    "# Increase figure size even further\n",
    "plt.figure(figsize=(20, 16))  # <-- Adjust the dimensions as needed\n",
    "\n",
    "# Create heatmap with adjustments (rest remains the same)\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\",\n",
    "            annot_kws={\"size\": 10},\n",
    "            linewidths=.5,\n",
    "            mask=mask)\n",
    "\n",
    "\n",
    "plt.title('Correlation Matrix of Food Security and Crime Statistics', fontsize=14)  # Increase title font size\n",
    "plt.xticks(rotation=45, ha='right', fontsize=12)  # Rotate x-axis labels and adjust font size\n",
    "plt.yticks(fontsize=12)  # Adjust y-axis label font size\n",
    "plt.tight_layout()  # Adjust layout to prevent overlapping labels\n",
    "plt.show()\n",
    "\n",
    "# Apply t-SNE to the correlation matrix\n",
    "tsne = TSNE(n_components=2, random_state=42)  # You can adjust n_components and random_state\n",
    "tsne_results = tsne.fit_transform(correlation_matrix)\n",
    "\n",
    "# Create a DataFrame for the t-SNE results\n",
    "tsne_df = pd.DataFrame(data=tsne_results, index=correlation_matrix.index, columns=['TSNE1', 'TSNE2'])\n",
    "\n",
    "# Visualize the t-SNE results\n",
    "plt.figure(figsize=(20, 16))\n",
    "for index, row in tsne_df.iterrows():\n",
    "    plt.scatter(row['TSNE1'], row['TSNE2'])  # Scatter plot for each point\n",
    "    plt.annotate(index, (row['TSNE1'], row['TSNE2']), fontsize=12)  # Increased fontsize to 12\n",
    "\n",
    "plt.title('t-SNE Visualization of Food Security and Crime Statistics (Labels on Points)')\n",
    "plt.xlabel('TSNE1')\n",
    "plt.ylabel('TSNE2')\n",
    "plt.show()\n",
    "\n",
    "# Standardize the data (important for SVD)\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(subset_df)\n",
    "\n",
    "# Apply SVD with desired number of components (e.g., 2)\n",
    "svd = TruncatedSVD(n_components=2, random_state=42)  # You can adjust n_components and random_state\n",
    "svd_results = svd.fit_transform(scaled_data)\n",
    "\n",
    "# Create a DataFrame for the SVD results\n",
    "svd_df = pd.DataFrame(data=svd_results, index=subset_df.index, columns=['SVD1', 'SVD2'])\n",
    "\n",
    "# Add labels to the scatterplot\n",
    "plt.figure(figsize=(20, 16))\n",
    "sns.scatterplot(x='SVD1', y='SVD2', data=svd_df)\n",
    "\n",
    "# Add labels for each data point (town in this case)\n",
    "for i, txt in enumerate(svd_df.index):\n",
    "    plt.annotate(txt, (svd_df['SVD1'][i], svd_df['SVD2'][i]), fontsize=8)\n",
    "\n",
    "plt.title('SVD Visualization of Food Security and Crime Statistics')\n",
    "plt.show()\n",
    "\n",
    "print(svd.explained_variance_ratio_.sum())\n",
    "\n"
   ],
   "metadata": {
    "id": "XXDlYOKdshnQ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Clustering with KMeans\n",
    "\n",
    "# Elbow Method for Optimal k\n",
    "wcss = []  # Within-cluster sum of squares\n",
    "for i in range(1, 11):  # Check for k values from 1 to 10\n",
    "    kmeans = KMeans(n_clusters=i, random_state=42)\n",
    "    kmeans.fit(svd_df[['SVD1', 'SVD2']])\n",
    "    wcss.append(kmeans.inertia_)  # Inertia is the WCSS value\n",
    "\n",
    "# Plot the elbow curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, 11), wcss, marker='o')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n",
    "\n",
    "# Based on the elbow curve, select the optimal k value\n",
    "# ... (visually inspect the plot to determine the 'elbow' point) ...\n",
    "\n",
    "# Given optimal k is 3 (based on the elbow curve)\n",
    "optimal_k = 3\n",
    "\n",
    "# Generate the cluster\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "svd_df['cluster'] = kmeans.fit_predict(svd_df[['SVD1', 'SVD2']])\n",
    "\n",
    "# Visualize clusters with different colors\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x='SVD1', y='SVD2', data=svd_df, hue='cluster', palette='viridis')\n",
    "plt.title('SVD Visualization with KMeans Clustering')\n",
    "plt.show()\n",
    "\n",
    "# Analyze cluster characteristics\n",
    "for cluster in svd_df['cluster'].unique():\n",
    "    print(f\"Cluster {cluster}:\")\n",
    "    print(svd_df[svd_df['cluster'] == cluster].index.tolist())\n",
    "    print(\"\\n\")\n",
    "\n",
    "score = silhouette_score(svd_df[['SVD1', 'SVD2']], svd_df['cluster'])\n",
    "print(f'Silhouette Score: {score}')\n",
    "\n",
    "# Create a DataFrame for the t-SNE results\n",
    "tsne_df = pd.DataFrame(data=tsne_results, index=correlation_matrix.index, columns=['TSNE1', 'TSNE2'])\n",
    "\n",
    "# Elbow Method for Optimal k (using t-SNE data)\n",
    "wcss = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, random_state=42)\n",
    "    kmeans.fit(tsne_df[['TSNE1', 'TSNE2']])  # Fit to t-SNE data\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the elbow curve (same as before)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, 11), wcss, marker='o')\n",
    "plt.title('Elbow Method for Optimal k (t-SNE Data)')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n",
    "\n",
    "# Given optimal k is 4 (based on the elbow curve)\n",
    "optimal_k = 4\n",
    "\n",
    "# Apply KMeans (using t-SNE data)\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "tsne_df['cluster'] = kmeans.fit_predict(tsne_df[['TSNE1', 'TSNE2']])\n",
    "\n",
    "# Add cluster assignments to the original DataFrame\n",
    "combined_crime_food_df['cluster'] = combined_crime_food_df['town'].map(tsne_df.set_index(correlation_matrix.index)['cluster'])\n",
    "\n",
    "# Analyze cluster characteristics (using combined_crime_food_df)\n",
    "for cluster in combined_crime_food_df['cluster'].unique():\n",
    "    print(f\"Cluster {cluster}:\")\n",
    "    print(combined_crime_food_df[combined_crime_food_df['cluster'] == cluster].index.tolist())\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Visualize the t-SNE results with clusters\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x='TSNE1', y='TSNE2', hue='cluster', data=tsne_df, palette='viridis')\n",
    "plt.title('t-SNE Visualization with KMeans Clustering')\n",
    "plt.show()\n",
    "\n",
    "# Calculate Silhouette Score (using t-SNE data)\n",
    "score = silhouette_score(tsne_df[['TSNE1', 'TSNE2']], tsne_df['cluster'])\n",
    "print(f'Silhouette Score (t-SNE Data): {score}')\n"
   ],
   "metadata": {
    "id": "iogWd0h8DE66"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 2. Scatter Plots:\n",
    "# Explore the relationship between specific food security indicators and crime rates.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Define the pairs of columns for scatter plots and explanations\n",
    "scatter_pairs = [\n",
    "    (('PovertyRate', 'Total Offenses'), \"Higher poverty rates are often associated with increased crime rates due to socioeconomic factors.\"),\n",
    "    (('MedianFamilyIncome', 'Crimes Against Property'), \"Areas with lower median family incomes might experience higher rates of property crimes due to economic disparities.\"),\n",
    "    (('Urban', 'Drug/Narcotic Offenses'), \"Urban areas may have higher drug-related offenses due to factors like population density and greater access to drug markets.\"),\n",
    "    (('TractHUNV', 'Motor Vehicle Theft'), \"Limited access to reliable vehicles might be correlated with the likelihood of motor vehicle theft.\"),\n",
    "    (('TractSNAP', 'Larceny/Theft Offenses'), \"Households facing food insecurity (indicated by SNAP benefits) might be more prone to larceny/theft offenses.\"),\n",
    "    (('TractLOWI', 'Motor Vehicle Theft'), \"Low income areas may have higher incidence of vehicle theft\"),\n",
    "    (('lakidshalf', 'Crimes Against Property'), \"Children with poor access to food (Kids population count beyond 1/2 mile from supermarket) and other resources may be more likely to commit crimes against property\"),\n",
    "    (('lakidshalf', 'Destruction/Damage/Vandalism of Property'), \"Children with poor access to food (Kids population count beyond 1/2 mile from supermarket) and other resources may be more likely to vandalize property or be unsupervised\"),\n",
    "    (('PovertyRate', 'Shoplifting'), \"As the poverty rate in an area increases, it may lead to higher rates of shoplifting\")\n",
    "]\n",
    "\n",
    "# Create a figure and a grid of subplots\n",
    "fig, axes = plt.subplots(nrows=len(scatter_pairs), ncols=1, figsize=(8, 8 * len(scatter_pairs)))\n",
    "\n",
    "\n",
    "# Create a custom colormap with a green-to-red gradient\n",
    "cmap_urban_rural_gradient = mcolors.LinearSegmentedColormap.from_list(\n",
    "    \"urban_rural_gradient\", [\"lightgreen\", \"red\"]  # Green for rural, red for urban\n",
    ")\n",
    "\n",
    "# Iterate through the scatter pairs and create subplots with trendlines and explanations\n",
    "for i, ((x_col, y_col), explanation) in enumerate(scatter_pairs):\n",
    "    ax = axes[i]  # Get the current subplot\n",
    "\n",
    "    scatter = ax.scatter(\n",
    "        combined_crime_food_df[x_col],\n",
    "        combined_crime_food_df[y_col],\n",
    "        c=combined_crime_food_df[\"Urban\"],\n",
    "        cmap=cmap_urban_rural_gradient,  # Use custom gradient colormap\n",
    "        label=\"Urban (Color)\",\n",
    "    )\n",
    "\n",
    "    # Trendline calculation and plotting\n",
    "    x = combined_crime_food_df[x_col]\n",
    "    y = combined_crime_food_df[y_col]\n",
    "\n",
    "    # Handle potential errors if NaN values are present in the trendline calculation\n",
    "    try:\n",
    "        # Get coefficients of linear regression\n",
    "        coefficients = np.polyfit(x, y, 1)\n",
    "        # Create a function representing the trendline\n",
    "        polynomial = np.poly1d(coefficients)\n",
    "        # Generate x values for the trendline plot\n",
    "        x_trendline = np.linspace(x.min(), x.max(), 100)\n",
    "        # Calculate corresponding y values based on trendline\n",
    "        y_trendline = polynomial(x_trendline)\n",
    "        # Plot the trendline\n",
    "        ax.plot(x_trendline, y_trendline, color='red', label='Trendline')\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating or plotting trendline for {x_col} vs {y_col}: {e}\")\n",
    "\n",
    "    ax.set_xlabel(x_col)\n",
    "    ax.set_ylabel(y_col)\n",
    "    ax.set_title(f'Relationship between {x_col} and {y_col}')\n",
    "    ax.text(0.05, 0.95, explanation, transform=ax.transAxes, va='top', fontsize=10, wrap=True)  # Add explanation text\n",
    "\n",
    "    # Add colorbar with custom label\n",
    "    cbar = fig.colorbar(scatter, ax=ax, label=\"Urban (Green: Rural, Red: Urban)\")\n",
    "\n",
    "# Adjust layout to prevent overlapping\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "JaWGRgHFDKR2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 3. Pairplot:\n",
    "# Visualize pairwise relationships between multiple variables.\n",
    "# You can select specific variables for the pairplot.\n",
    "numeric_cols = ['LowIncomeTracts', 'PovertyRate', 'Total Offenses',\n",
    "                 'MedianFamilyIncome', 'Larceny/Theft Offenses', 'Assault Offenses',\n",
    "                 'Homicide Offenses', 'Drug/Narcotic Offenses',\n",
    "                 'TractKids', 'Urban', 'TractHUNV', 'TractSNAP']\n",
    "combined_crime_food_df[numeric_cols] = combined_crime_food_df[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "sns.pairplot(combined_crime_food_df[numeric_cols], kind='reg')\n",
    "plt.show()\n",
    "\n",
    "# Define thresholds for categorization\n",
    "combined_crime_food_df['Urban_Category'] = pd.cut(combined_crime_food_df['Urban'], bins=[-0.1, 0.3, 0.7, 1.1],\n",
    "                                                  labels=['Rural', 'Mixed', 'Urban'])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x='LowIncomeTracts', y='Total Offenses', data=combined_crime_food_df, hue='Urban_Category')\n",
    "plt.xlabel('Low Income Tracts')\n",
    "plt.ylabel('Total Offenses')\n",
    "plt.title('Relationship between Low Income Tracts and Total Offenses by Urban Category')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "# Use the 'c' parameter to map the data to color directly, and specify a colormap\n",
    "scatter = plt.scatter(x='Population', y='Total Offenses', c='Urban', data=combined_crime_food_df, cmap='coolwarm')\n",
    "plt.xlabel('Population')\n",
    "plt.ylabel('Total Offenses')\n",
    "plt.title('Relationship between Population and Total Offenses by Percentage Urban')\n",
    "# Add the colorbar, referencing the scatter plot\n",
    "plt.colorbar(scatter, label='Percentage Urban')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "NBVpyF1RDNyl"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 4. Histogram:\n",
    "# Analyze the distribution of crime rates or food security indicators.\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(combined_crime_food_df['Total Offenses'], bins=20)\n",
    "plt.xlabel('Total Offenses')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Total Offenses')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "0Zs6qn5qDROw"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 5. Boxplots:\n",
    "# Compare the distribution of crime rates across different categories of food security (e.g., urban vs. rural).\n",
    "# Calculate percentiles of 'Total Offenses' for each 'Urban_Category'\n",
    "urban_offenses_dist = combined_crime_food_df.groupby('Urban_Category')['Total Offenses'].quantile(np.linspace(0, 1, 100)).reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "urban_offenses_dist.columns = ['Urban_Category', 'Quantile', 'Total_Offenses']\n",
    "\n",
    "# Create a line plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.lineplot(x='Total_Offenses', y='Quantile', hue='Urban_Category', data=urban_offenses_dist)\n",
    "plt.xlabel('Total Offenses')\n",
    "plt.ylabel('Quantile')\n",
    "plt.title('Distribution of Total Offenses across Urban/Rural Areas')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxenplot(x='Urban_Category', y='Total Offenses', data=combined_crime_food_df, showfliers=False)\n",
    "plt.xlabel('Urban/Rural Category')\n",
    "plt.ylabel('Total Offenses')\n",
    "plt.title('Distribution of Total Offenses across Urban/Rural Areas')\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels if needed\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Apply jitter to x-values before plotting\n",
    "# Extract x-values as a NumPy array\n",
    "x_values = combined_crime_food_df['Urban_Category'].cat.codes.astype(float)\n",
    "\n",
    "# Calculate jitter values\n",
    "x_jitter_values = np.random.uniform(-0.1, 0.1, size=x_values.shape)\n",
    "\n",
    "# Apply jitter\n",
    "jittered_x_values = x_values + x_jitter_values\n",
    "\n",
    "sns.scatterplot(x=jittered_x_values,  # Use jittered x-values\n",
    "                y='Total Offenses',\n",
    "                data=combined_crime_food_df,\n",
    "                alpha=0.5,\n",
    "                hue='Urban_Category',  # Use hue for color mapping\n",
    "                legend='full')  # Show the legend\n",
    "\n",
    "# Update x-axis tick labels to original categories\n",
    "plt.xticks(ticks=np.unique(x_values), labels=combined_crime_food_df['Urban_Category'].cat.categories)\n",
    "\n",
    "plt.xlabel('Urban/Rural Category')\n",
    "plt.ylabel('Total Offenses')\n",
    "plt.title('Distribution of Total Offenses across Urban/Rural Areas')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "cT6_d_gzDTVt"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Select rows with at least one non-NaN value\n",
    "combined_crime_food_df = combined_crime_food_df.dropna(how='all')\n",
    "# Select only the columns between \"Urban\" and \"TractSNAP\" for the features\n",
    "X = combined_crime_food_df.loc[:, 'Urban':'TractSNAP']\n",
    "\n",
    "# Convert all columns to numeric, coercing errors to NaN\n",
    "X = X.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Ensure only numerical data is included in the features\n",
    "X = X.select_dtypes(include=np.number)\n",
    "\n",
    "# Print the selected feature DataFrame\n",
    "print(\"Features DataFrame (X):\")\n",
    "print(X.head())  # Display the first few rows of the DataFrame\n",
    "print(X.dtypes)  # Confirm the data types of each column\n",
    "\n",
    "# Get the target columns from \"Total Offenses\" to the last column, EXCLUDING 'Urban_Category'\n",
    "target_columns = combined_crime_food_df.columns[68:-1]  # Exclude the last column ('Urban_Category')\n",
    "\n",
    "# Remove any target columns that are not numeric\n",
    "target_columns = [col for col in target_columns if pd.api.types.is_numeric_dtype(combined_crime_food_df[col])]\n",
    "\n",
    "# Initialize a dictionary to store results for all target variables\n",
    "results_matrix = pd.DataFrame(columns=['Model', 'Target Variable', 'R-squared', 'RMSE', 'Cross-Validation R-squared'])\n",
    "\n",
    "# Function to train and evaluate models\n",
    "def evaluate_models(X, y, target_name):\n",
    "    global results_matrix\n",
    "    # Drop rows with NaN values in the target variable\n",
    "    X_filtered = X.loc[y.notna()]  # Select rows from X where y is not NaN\n",
    "    y_filtered = y.dropna()  # Remove NaN values from y\n",
    "\n",
    "    if X_filtered.empty or y_filtered.empty:\n",
    "       print(f\"Skipping {target_name} due to insufficient data after removing NaN values.\")\n",
    "       return  # Skip this target variable if data is insufficient\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_filtered, y_filtered, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Linear Regression\n",
    "    linear_reg = LinearRegression()\n",
    "    linear_reg.fit(X_train, y_train)\n",
    "    y_pred_linear = linear_reg.predict(X_test)\n",
    "    cv_scores_linear = cross_val_score(linear_reg, X, y, cv=5, scoring='r2')\n",
    "\n",
    "    results_matrix.loc[len(results_matrix)] = [\n",
    "        'Linear Regression',\n",
    "        target_name,\n",
    "        r2_score(y_test, y_pred_linear),\n",
    "        mean_squared_error(y_test, y_pred_linear, squared=False),\n",
    "        cv_scores_linear.mean()\n",
    "    ]\n",
    "    print(f\"Results after Linear Regression for {target_name}:\")\n",
    "    print(results_matrix.tail(1))  # Print the latest row of results\n",
    "\n",
    "    # Random Forest Regression\n",
    "    rf_reg = RandomForestRegressor(random_state=42)\n",
    "    rf_reg.fit(X_train, y_train)\n",
    "    y_pred_rf = rf_reg.predict(X_test)\n",
    "    cv_scores_rf = cross_val_score(rf_reg, X, y, cv=5, scoring='r2')\n",
    "\n",
    "    results_matrix.loc[len(results_matrix)] = [\n",
    "        'Random Forest Regression',\n",
    "        target_name,\n",
    "        r2_score(y_test, y_pred_rf),\n",
    "        mean_squared_error(y_test, y_pred_rf, squared=False),\n",
    "        cv_scores_rf.mean()\n",
    "    ]\n",
    "    print(f\"Results after Random Forest Regression for {target_name}:\")\n",
    "    print(results_matrix.tail(1))  # Print the latest row of results\n",
    "\n",
    "    # Decision Tree Regression\n",
    "    dt_reg = DecisionTreeRegressor(random_state=42)\n",
    "    dt_reg.fit(X_train, y_train)\n",
    "    y_pred_dt = dt_reg.predict(X_test)\n",
    "    cv_scores_dt = cross_val_score(dt_reg, X, y, cv=5, scoring='r2')\n",
    "\n",
    "    results_matrix.loc[len(results_matrix)] = [\n",
    "        'Decision Tree Regression',\n",
    "        target_name,\n",
    "        r2_score(y_test, y_pred_dt),\n",
    "        mean_squared_error(y_test, y_pred_dt, squared=False),\n",
    "        cv_scores_dt.mean()\n",
    "    ]\n",
    "    print(f\"Results after Decision Tree Regression for {target_name}:\")\n",
    "    print(results_matrix.tail(1))  # Print the latest row of results\n",
    "\n",
    "    # Support Vector Regression\n",
    "    svr_reg = SVR(kernel='rbf')  # You can experiment with different kernels\n",
    "    svr_reg.fit(X_train, y_train)\n",
    "    y_pred_svr = svr_reg.predict(X_test)\n",
    "    cv_scores_svr = cross_val_score(svr_reg, X, y, cv=5, scoring='r2')\n",
    "\n",
    "    results_matrix.loc[len(results_matrix)] = [\n",
    "        'Support Vector Regression',\n",
    "        target_name,\n",
    "        r2_score(y_test, y_pred_svr),\n",
    "        mean_squared_error(y_test, y_pred_svr, squared=False),\n",
    "        cv_scores_svr.mean()\n",
    "    ]\n",
    "    print(f\"Results after Support Vector Regression for {target_name}:\")\n",
    "    print(results_matrix.tail(1))\n",
    "\n",
    "    # XGBoost Regression\n",
    "    xgb_reg = XGBRegressor(random_state=42)\n",
    "    xgb_reg.fit(X_train, y_train)\n",
    "    y_pred_xgb = xgb_reg.predict(X_test)\n",
    "    cv_scores_xgb = cross_val_score(xgb_reg, X, y, cv=5, scoring='r2')\n",
    "\n",
    "    results_matrix.loc[len(results_matrix)] = [\n",
    "        'XGBoost Regression',\n",
    "        target_name,\n",
    "        r2_score(y_test, y_pred_xgb),\n",
    "        mean_squared_error(y_test, y_pred_xgb, squared=False),\n",
    "        cv_scores_xgb.mean()\n",
    "    ]\n",
    "    print(f\"Results after XGBoost Regression for {target_name}:\")\n",
    "    print(results_matrix.tail(1))\n",
    "\n",
    "\n",
    "    # K-Nearest Neighbors Regression\n",
    "    knn_reg = KNeighborsRegressor(n_neighbors=5)  # You can tune the number of neighbors\n",
    "    knn_reg.fit(X_train, y_train)\n",
    "    y_pred_knn = knn_reg.predict(X_test)\n",
    "    cv_scores_knn = cross_val_score(knn_reg, X, y, cv=5, scoring='r2')\n",
    "\n",
    "    results_matrix.loc[len(results_matrix)] = [\n",
    "        'KNN Regression',\n",
    "        target_name,\n",
    "        r2_score(y_test, y_pred_knn),\n",
    "        mean_squared_error(y_test, y_pred_knn, squared=False),\n",
    "        cv_scores_knn.mean()\n",
    "    ]\n",
    "    print(f\"Results after KNN Regression for {target_name}:\")\n",
    "    print(results_matrix.tail(1))\n",
    "\n",
    "# Iterate over each target column and evaluate models\n",
    "for target_name in target_columns:\n",
    "    y = combined_crime_food_df[target_name]\n",
    "    evaluate_models(X, y, target_name)\n",
    "\n",
    "# Display the final results matrix\n",
    "print(\"\\nFinal Results Matrix:\")\n",
    "print(results_matrix)\n"
   ],
   "metadata": {
    "collapsed": true,
    "id": "W0OCjrxmUtpC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def visualize_model_performance(results_matrix):\n",
    "\n",
    "    # Find the best model for each target variable based on R-squared\n",
    "    best_models = results_matrix.loc[\n",
    "        results_matrix.groupby('Target Variable')['R-squared'].idxmax()\n",
    "    ]\n",
    "\n",
    "    # Reshape the data for visualization\n",
    "    model_r2_pivot = best_models.pivot(\n",
    "        index='Model', columns='Target Variable', values='R-squared'\n",
    "    )\n",
    "\n",
    "    # Create a custom colormap (red to green) for R-squared values\n",
    "    r2_colormap = mcolors.LinearSegmentedColormap.from_list(\n",
    "        \"rg\", [\"red\", \"orange\", \"yellow\", \"lightgreen\"], N=256\n",
    "    )\n",
    "\n",
    "    # Create the heatmap using seaborn with text offsets\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    ax = sns.heatmap(\n",
    "        model_r2_pivot,\n",
    "        annot=True,\n",
    "        cmap=r2_colormap,\n",
    "        fmt=\".2f\",\n",
    "        annot_kws={'size': 10}  # Adjust annotation font size\n",
    "    )\n",
    "\n",
    "    # Iterate through annotations to apply vertical offsets\n",
    "    for t in ax.texts:\n",
    "        t.set_y(t.get_position()[1] + 0.01)  # Adjust offset as needed\n",
    "\n",
    "    plt.title('Maximum R-squared for Each Target Variable by Model')\n",
    "    plt.xlabel('Target Variable')\n",
    "    plt.ylabel('Model')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Melt the pivot table for bar chart and stripplot (using model_r2_pivot)\n",
    "    melted_df = model_r2_pivot.reset_index().melt(id_vars=['Model'],\n",
    "                                                value_vars=model_r2_pivot.columns,\n",
    "                                                var_name='Target Variable',\n",
    "                                                value_name='R-squared')\n",
    "\n",
    "    # Create the grouped bar chart (using melted_df)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Target Variable', y='R-squared', hue='Model', data=melted_df)\n",
    "    plt.title('Model Performance (R-squared) for Different Crime Types')\n",
    "    plt.xlabel('Crime Type')\n",
    "    plt.ylabel('R-squared')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Create the dot plot (stripplot) (using melted_df)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.stripplot(x='Target Variable', y='R-squared', hue='Model', data=melted_df, jitter=True, dodge=True)\n",
    "    plt.title('Model Performance (R-squared) for Different Crime Types')\n",
    "    plt.xlabel('Crime Type')\n",
    "    plt.ylabel('R-squared')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Call the function to visualize the results\n",
    "visualize_model_performance(results_matrix)"
   ],
   "metadata": {
    "id": "xmOEhmxAf3NV"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def visualize_model_performance_2(results_matrix):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "\n",
    "    # Check if required columns are present in results_matrix\n",
    "    if 'R-squared' not in results_matrix.columns or 'Target Variable' not in results_matrix.columns or 'Model' not in results_matrix.columns:\n",
    "        raise ValueError(\"results_matrix must contain 'R-squared', 'Target Variable', and 'Model' columns.\")\n",
    "\n",
    "    # Filter R-squared values to the range -2 to 1\n",
    "    filtered_results = results_matrix[(results_matrix['R-squared'] >= -2) & (results_matrix['R-squared'] <= 1)].copy()\n",
    "\n",
    "    # Ensure 'Target Variable' is treated as a categorical variable with the correct order\n",
    "    crime_type_order = filtered_results['Target Variable'].unique()  # Get unique crime types\n",
    "    filtered_results['Target Variable'] = pd.Categorical(\n",
    "        filtered_results['Target Variable'],\n",
    "        categories=crime_type_order,\n",
    "        ordered=True\n",
    "    )\n",
    "\n",
    "    # Sort the entire DataFrame by 'Target Variable'\n",
    "    filtered_results = filtered_results.sort_values(by='Target Variable')\n",
    "\n",
    "    # Start plotting\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Loop through each model and plot\n",
    "    for model in filtered_results['Model'].unique():\n",
    "        model_data = filtered_results[filtered_results['Model'] == model]\n",
    "\n",
    "        # Plot with lines only between sequential points for the same model\n",
    "        plt.plot(\n",
    "            model_data['Target Variable'],\n",
    "            model_data['R-squared'],\n",
    "            label=model,\n",
    "            marker='o',\n",
    "            linestyle='-',\n",
    "            alpha=0.7  # Optional transparency for clarity\n",
    "        )\n",
    "\n",
    "    plt.title('Model Performance (R-squared) for Different Crime Types (Zoomed)')\n",
    "    plt.xlabel('Crime Type')\n",
    "    plt.ylabel('R-squared')\n",
    "    plt.ylim([-2, 1])  # Set Y-axis limits\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "    plt.legend(title='Model', loc='upper right')  # Adjust legend position for visibility\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_model_performance_2(results_matrix)"
   ],
   "metadata": {
    "id": "IKns1RojwhI1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the Data above, we can see a few important things. Firstly, note that Linear Regression and Descision Tree Regression (both simpler models) tended to perform better on crimes for which there was less data. This makes sense, because many of those will be 0 or a constant value all the way down, making the function to predict extremely simple. This leads to high accuracy of these types of models, but may indicate this data is not very useful without more crime data for less common crimes, since things like computer hacking, sports tampering, and betting / wagering are uncommon offenses. KNN regression appears to be the highest performer"
   ],
   "metadata": {
    "id": "sV4r_OOUac9X"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def filter_matching_columns(df1, df2):\n",
    "    # Get common columns between both dataframes\n",
    "    common_columns = df1.columns.intersection(df2.columns)\n",
    "\n",
    "    # Filter df1 to keep only common columns\n",
    "    filtered_df = df1[common_columns]\n",
    "\n",
    "    # Fill any empty cells (NaN values) with 0\n",
    "    filtered_df = filtered_df.fillna(0)\n",
    "\n",
    "    return filtered_df"
   ],
   "metadata": {
    "id": "27K0YJVGwOzF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pick the best performing model, and train it again on the dataset"
   ],
   "metadata": {
    "id": "HQs_yjx0YRJe"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# CT Data Cleanup\n",
    "combined_crime_food_df.to_excel('CT_combined_crime_food_df.xlsx', index=False)\n",
    "combined_crime_food_df.to_csv('CT_combined_crime_food_df.csv', index=False)\n",
    "\n",
    "# Austin Data Cleanup\n",
    "filtered_austin_df = filter_matching_columns(Austin_County_Food_df, combined_crime_food_df)\n",
    "filtered_austin_df.to_excel('filtered_austin_food_df.xlsx', index=False)\n",
    "filtered_austin_df.to_csv('filtered_austin_food_df.csv', index=False)\n",
    "\n",
    "# Selecting X and y from combined_crime_food_df\n",
    "X_combined = combined_crime_food_df.loc[:, \"Urban\":\"TractSNAP\"]\n",
    "y_combined = combined_crime_food_df[\"Total Offenses\"]\n",
    "\n",
    "# Selecting X from filtered_austin_df\n",
    "X_austin = filtered_austin_df.loc[:, \"Urban\":\"TractSNAP\"]\n",
    "\n",
    "print(X_combined.shape, y_combined.shape, X_austin.shape)\n",
    "\n",
    "print(X_combined.isnull().sum())\n",
    "print(y_combined.isnull().sum())\n",
    "print(X_austin.isnull().sum())\n"
   ],
   "metadata": {
    "id": "CR43EYTdYQhg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Split the CT data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y_combined, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Create a KNN Regressor\n",
    "knn_model = KNeighborsRegressor(n_neighbors=4)  # You can tune this hyperparameter\n",
    "\n",
    "# Train the model on the training data\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "y_pred_test = knn_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred_test)\n",
    "r2 = r2_score(y_test, y_pred_test)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared Score:\", r2)\n",
    "\n",
    "# Predict crime rates for Austin\n",
    "y_pred_austin = knn_model.predict(X_austin)\n",
    "\n",
    "# Add the predicted crime rates to the Austin DataFrame\n",
    "filtered_austin_df['Predicted_Total_Crime'] = y_pred_austin\n",
    "\n",
    "# Reorder columns to move 'Predicted_Total_Crime' to the third position\n",
    "cols = list(filtered_austin_df.columns)\n",
    "cols.insert(2, cols.pop(cols.index('Predicted_Total_Crime')))\n",
    "filtered_austin_df = filtered_austin_df[cols]\n",
    "\n",
    "# Output the DataFrame to an Excel spreadsheet\n",
    "filtered_austin_df.to_excel('austin_with_predictions.xlsx', index=False)\n",
    "\n",
    "\n",
    "# Visualize the actual vs. predicted crime rates for CT\n",
    "plt.scatter(y_test, y_pred_test, alpha=0.5)\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')\n",
    "plt.xlabel('Actual Crime Rate')\n",
    "plt.ylabel('Predicted Crime Rate')\n",
    "plt.title('Actual vs. Predicted Crime Rates (CT Data)')\n",
    "plt.show()\n",
    "\n",
    "# You can now analyze the predicted crime rates for Austin\n",
    "print(y_pred_austin)"
   ],
   "metadata": {
    "id": "LaEFldDLK2jZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_predicted_crime_rates(df, predicted_rates, relevant_columns):\n",
    "\n",
    "    # Create subplots for each relevant column\n",
    "    num_plots = len(relevant_columns)\n",
    "    fig, axes = plt.subplots(nrows=num_plots, ncols=1, figsize=(8, 6 * num_plots))\n",
    "\n",
    "    # Ensure axes is always iterable, even for a single plot\n",
    "    if num_plots == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    # Iterate through relevant columns and create plots\n",
    "    for i, column in enumerate(relevant_columns):\n",
    "        ax = axes[i]  # Get the current subplot axis\n",
    "\n",
    "        # Scatter plot\n",
    "        ax.scatter(df[column], predicted_rates, alpha=0.5, label='Data')\n",
    "\n",
    "        # Fit a regression line\n",
    "        x = df[column].values.reshape(-1, 1)\n",
    "        y = predicted_rates\n",
    "        model = LinearRegression()\n",
    "        model.fit(x, y)\n",
    "        y_pred = model.predict(x)\n",
    "\n",
    "        # Plot the regression line\n",
    "        ax.plot(df[column], y_pred, color='red', label='Prediction Line')\n",
    "\n",
    "        # Set labels and title\n",
    "        ax.set_xlabel(column)\n",
    "        ax.set_ylabel('Predicted Crime Rate')\n",
    "        ax.set_title(f'Predicted Crime Rates for Austin vs. {column}')\n",
    "        ax.legend()\n",
    "\n",
    "    # Adjust layout and display plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "relevant_cols = ['Pop2010', 'Urban', 'TractLOWI', 'TractHUNV', 'TractKids', 'PovertyRate', 'MedianFamilyIncome', 'TractSNAP', 'lapophalf', 'lalowihalf', 'lakidshalf']  # Example relevant columns\n",
    "plot_predicted_crime_rates(filtered_austin_df, y_pred_austin, relevant_cols)"
   ],
   "metadata": {
    "id": "O6jtxqQWM_Sf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Input\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure data is in NumPy array format with correct dtype\n",
    "X_combined = np.array(X_combined, dtype=np.float32)\n",
    "y_combined = np.array(y_combined, dtype=np.float32)\n",
    "X_austin = np.array(X_austin, dtype=np.float32)\n",
    "\n",
    "# Split the CT data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y_combined, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a neural network model\n",
    "def create_model(input_dim, learning_rate=0.001, num_hidden_layers=2, neurons_per_layer=64):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(input_dim,)))\n",
    "    for _ in range(num_hidden_layers):\n",
    "        model.add(Dense(neurons_per_layer, activation='relu'))\n",
    "    model.add(Dense(1))  # Single output for regression\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Define hyperparameters\n",
    "input_dim = X_combined.shape[1]\n",
    "learning_rate = 0.001\n",
    "num_hidden_layers = 3\n",
    "neurons_per_layer = 64\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "\n",
    "# Create and train the model\n",
    "model = create_model(input_dim, learning_rate, num_hidden_layers, neurons_per_layer)\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "                    batch_size=batch_size, epochs=epochs, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_test_nn = model.predict(X_test)\n",
    "mse_nn = mean_squared_error(y_test, y_pred_test_nn)\n",
    "r2_nn = r2_score(y_test, y_pred_test_nn)\n",
    "print(\"Neural Network Mean Squared Error:\", mse_nn)\n",
    "print(\"Neural Network R-squared Score:\", r2_nn)\n",
    "\n",
    "# Predict crime rates for Austin\n",
    "y_pred_austin_nn = model.predict(X_austin)\n",
    "\n",
    "# Visualize the actual vs. predicted crime rates for CT\n",
    "plt.scatter(y_test, y_pred_test_nn, alpha=0.5)\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')\n",
    "plt.xlabel('Actual Crime Rate')\n",
    "plt.ylabel('Predicted Crime Rate')\n",
    "plt.title('Actual vs. Predicted Crime Rates (CT Data) - Neural Network')\n",
    "plt.show()\n",
    "\n",
    "# Analyze the predicted crime rates for Austin\n",
    "print(y_pred_austin_nn)\n",
    "\n",
    "relevant_cols = ['Pop2010', 'Urban', 'TractLOWI', 'TractHUNV', 'TractKids', 'PovertyRate', 'MedianFamilyIncome', 'TractSNAP', 'lapophalf', 'lalowihalf', 'lakidshalf']  # Example relevant columns\n",
    "plot_predicted_crime_rates(filtered_austin_df, y_pred_austin_nn, relevant_cols)"
   ],
   "metadata": {
    "id": "HjRwjBYWu_5-"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
